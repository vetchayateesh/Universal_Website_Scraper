# ğŸŒ Universal Website Scraper

A **Universal Website Scraper** built to extract structured, section-aware content from **any website** â€” including **static pages and JavaScript-rendered sites**.  
It returns clean **JSON output** and provides a **simple frontend viewer** to explore and download the scraped data.

This project was built as an **MVP Full-Stack Assignment** focusing on robustness, clarity, and real-world scraping scenarios.

---

## ğŸš€ Features

- ğŸ” Scrapes **static & JS-rendered websites**
- ğŸ§  **Automatic fallback** from static scraping â†’ Playwright rendering
- ğŸ–±ï¸ Handles **click flows** (tabs, â€œLoad moreâ€, show more buttons)
- ğŸ“œ Supports **scrolling & pagination** (depth â‰¥ 3)
- ğŸ§© Groups content into **logical sections** (Hero, Nav, Footer, FAQ, etc.)
- ğŸ“¦ Outputs **section-aware structured JSON**
- ğŸ–¥ï¸ Minimal **frontend UI** to:
  - Input URL
  - View parsed sections
  - Expand JSON per section
  - Download full JSON result
- ğŸ›¡ï¸ Graceful error handling with partial results

---

## ğŸ› ï¸ Tech Stack

### Backend
- **Python 3.10+**
- **FastAPI**
- **httpx / requests**
- **BeautifulSoup / lxml**
- **Playwright (Python)** â€“ for JS rendering
- **Uvicorn**

### Frontend
- Minimal HTML / Jinja2 based UI  
- JSON viewer with expandable sections

---

## ğŸ“‚ Project Structure

```bash
Universal_Website_Scraper/
â”‚
â”œâ”€â”€ app/ # Backend source code
â”œâ”€â”€ templates/ # Frontend templates
â”œâ”€â”€ static/ # UI assets (if any)
â”œâ”€â”€ run.sh # One-command run script
â”œâ”€â”€ requirements.txt # Dependencies
â”œâ”€â”€ README.md # Project documentation
â”œâ”€â”€ design_notes.md # Design & strategy explanation
â”œâ”€â”€ capabilities.json # Implemented feature checklist
â””â”€â”€ main.py # Application entry point
```

---
