# Universal Website Scraper ğŸš€

**Universal Website Scraper** is a full-stack web scraping solution designed to extract clean, structured, and section-aware content from any website â€” whether itâ€™s static HTML or JavaScript-rendered. The project provides both a powerful backend API and a minimal frontend viewer so users can explore and download scraped results as structured JSON. :contentReference[oaicite:0]{index=0}

---

## ğŸ§  Features

âœ” Scrapes both **static and JavaScript-rendered websites**  
âœ” Automatic fallback from static scraping to Playwright rendering  
âœ” Handles interactive flows (e.g., click â€œLoad moreâ€, navigation tabs)  
âœ” Supports scrolling and pagination (deep scraping with depth â‰¥ 3)  
âœ” Outputs **section-aware structured JSON**  
âœ” Includes a **simple UI** to input URLs, view parsed sections, and download results  
âœ” Robust error handling with partial result recovery :contentReference[oaicite:1]{index=1}

---

## ğŸ§± Tech Stack

### Backend
- **Python 3.10+**
- **FastAPI** â€“ REST API framework  
- **httpx / requests** â€“ HTTP client  
- **BeautifulSoup / lxml** â€“ Parsing HTML  
- **Playwright (Python)** â€“ Browser automation for dynamic pages  
- **Uvicorn** â€“ ASGI server  

### Frontend
- Minimal **HTML + Jinja2** UI  
- JSON viewer for exploring scraped data :contentReference[oaicite:2]{index=2}

---

## ğŸ“‚ Project Structure

```bash
Universal_Website_Scraper/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ main.py                  # FastAPI app entry point
â”‚   â”‚   â”œâ”€â”€ config.py                # App configuration & constants
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ api/                     # API routes
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ scraper_routes.py    # Scraping endpoints
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ core/                    # Core scraping logic
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ static_scraper.py    # Static HTML scraping
â”‚   â”‚   â”‚   â”œâ”€â”€ dynamic_scraper.py   # Playwright-based scraping
â”‚   â”‚   â”‚   â”œâ”€â”€ interaction.py       # Scroll, click, pagination handling
â”‚   â”‚   â”‚   â””â”€â”€ fallback.py          # Static â†’ Dynamic fallback logic
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ services/                # Business logic layer
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ scraper_service.py   # Orchestrates full scraping flow
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ schemas/                 # Request/response models
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ scraper_schema.py    # Pydantic models
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ utils/                   # Helper utilities
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ html_parser.py       # Section-aware parsing
â”‚   â”‚   â”‚   â”œâ”€â”€ text_cleaner.py      # Cleans extracted text
â”‚   â”‚   â”‚   â””â”€â”€ logger.py            # Centralized logging
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ exceptions/              # Custom exceptions
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â””â”€â”€ scraper_exceptions.py
â”‚   â”‚
â”‚   â””â”€â”€ tests/                       # Backend tests
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ test_scraper.py
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ templates/
â”‚   â”‚   â””â”€â”€ index.html               # Main UI template
â”‚   â”‚
â”‚   â”œâ”€â”€ static/
â”‚   â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â”‚   â””â”€â”€ styles.css
â”‚   â”‚   â”œâ”€â”€ js/
â”‚   â”‚   â”‚   â””â”€â”€ app.js               # API calls & rendering
â”‚   â”‚   â””â”€â”€ assets/
â”‚   â”‚
â”‚   â””â”€â”€ README.md                    # Frontend documentation
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ ARCHITECTURE.md              # System design & flow
â”‚   â”œâ”€â”€ API_REFERENCE.md             # Endpoint documentation
â”‚   â””â”€â”€ DESIGN_NOTES.md              # Design decisions
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run.sh                       # Linux / macOS runner
â”‚   â””â”€â”€ run.ps1                      # Windows runner
â”‚
â”œâ”€â”€ capabilities.json                # Feature capability definitions
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md                        # Main project README
â””â”€â”€ LICENSE                          # (Optional but recommended)

```

---

## ğŸ› ï¸ Installation

### 1. Clone the repository

```bash
git clone https://github.com/vetchayateesh/Universal_Website_Scraper.git
cd Universal_Website_Scraper
```

###  2. Create and activate a virtual environment

```bash
python -m venv venv
source venv/bin/activate       # macOS / Linux
.\venv\Scripts\activate        # Windows
```

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

### 4. Install Playwright

```bash
playwright install
```
---
## Running the Project

### Start the API server
```bash
uvicorn backend.main:app --reload
```
---
## Access the UI
```bash
http://localhost:8000
```
---
## ğŸ” Usage Overview

Enter the URL of the website you want to scrape.

- The scraper intelligently determines whether to use static HTML scraping or Playwright for dynamic content.

- Extracted content is structured into logical sections like Hero, Navigation, Footer, FAQ, etc.

- View the JSON result in the built-in viewer and download if needed.
---
## ğŸ“¦ Output Format
The scraper returns a JSON object with:

- Website metadata

- Structured text grouped by logical sections

- Option to export the data for further analysis or integration

## Example snippet:
```bash
{
  "url": "https://example.com",
  "sections": {
    "header": {...},
    "main": {...},
    "footer": {...}
  },
  "timestamp": "2025-12-XXTXX:XX:XXZ"
}

```
---
## ğŸ¤ Contributing

Contributions are welcome! You can:

- Report bugs

- Suggest features

- Submit pull requests

Please follow the repositoryâ€™s issue templates and coding standards.
---

## ğŸ“„ License

This project does not yet specify a license. Consider adding a license file (e.g., MIT, Apache-2.0) to clarify usage rights.
---

## ğŸ“Œ About

Universal Website Scraper was developed as an MVP full-stack assignment with a focus on robustness, clarity, and real-world usability. Itâ€™s ideal for anyone looking to build powerful scraping tools backed by modern Python frameworks.
---
